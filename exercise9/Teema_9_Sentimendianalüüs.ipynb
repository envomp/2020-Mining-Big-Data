{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimendianalüüs\n",
    "\n",
    "\n",
    "## Sisukord\n",
    "\n",
    "* [Ülesanne 9 ja Boonusülesanne](#9)\n",
    "* [Andmete lugemine](#lugemine)\n",
    "* [Sõnade relevantsuse hindamine](#relevants)\n",
    "* [Logistiline regressioon dokumentide klassifitseerimiseks](#klf)\n",
    "\n",
    "Põhineb S.Raschka *Python Machine Learnig* raamatu \n",
    "[peatükil 8](https://github.com/rasbt/python-machine-learning-book/blob/master/code/ch08) ([*MIT litsents*](https://github.com/rasbt/python-machine-learning-book/blob/master/LICENSE.txt))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='9'></a>\n",
    "## Ülesanne 9\n",
    "\n",
    "Koostada oma originaalandmetest (vt ka Moodle sektsiooni *Iseseisev töö*) .csv fail. Kirjeldada andmestikku lühidalt ja dokumenteerida andmestiku atribuudid eraldi failis kujul **atribuudi_nimi, andmetüüp (tõeväärtus, kategooria, täisarv, reaalarv, tekst, kuupäev,...), selgitus**. Kui on olemas klass või arvväärtus, mida soovime ennustada, siis dokumenteerida ka see.\n",
    "\n",
    "Või kui originaalandmed puuduvad, siis selle ülesande punktide saamiseks:\n",
    "Laadida Project Gutenberg lehelt alla Goethe Fausti http://www.gutenberg.org/ebooks/14591 ja Iliase http://www.gutenberg.org/ebooks/6130 tekstifailid (*plain text, utf-8*). Jagada need failid salmideks, kus salme eraldavad kaks reavahetust ja salmipikkus on üle saja ja alla tuhande tähemärgi, näiteks:\n",
    "\n",
    "`text = file.read()\n",
    "salmid = [t for t in text.split(\"\\n\\n\") if 100 < len(t) < 1000]`\n",
    "\n",
    "Koostage .csv fail, kus read on salmid ja esimeseks veeruks on salmi tekst ning teiseks veeruks on salmi allikas (Ilias või Faust).\n",
    "\n",
    "\n",
    "## Ülesanne 9.BOONUS\n",
    "\n",
    "\n",
    "Viige oma originaalandmestik sellisele kujule, mida ka teised tudengid saaksid kasutada ja laadige see Moodlesse üles. Vaadake, et teil oleksid seaduslikud  ja tööalased õigused neid andmeid jagada. Anonümiseerige isikuandmed (kui on) st asendage nimed ja isikukoodid meelevaldsete koodidega.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='lugemine'></a>\n",
    "## Andmete lugemine\n",
    "\n",
    "Andmete töötluseks ja movie_data.csv faili koostamiseks on eraldi ipynb märkmik."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>The House of the Spirits is a gripping tale of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I have very fond memories of this film, as I s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Seeing this movie always reminds me of what I ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  The House of the Spirits is a gripping tale of...          1\n",
       "1  I have very fond memories of this film, as I s...          1\n",
       "2  Seeing this movie always reminds me of what I ...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eeldus: movie_data.csv on tekitatud\n",
    "\n",
    "df = pd.read_csv('./movie_data.csv')\n",
    "df.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='relevants'></a>\n",
    "## Sõnade relevantsuse hindamine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Terminisagedus $tf(t, d)$ näitab kui mitu korda termin $t$ esineb dokumendis $d$. Klassi [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) transformeerimismeetod `fit_transform(X)` teisendab tekstimassiivi $t \\times d$ terminisagedusmassiiviks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 5, 'sun': 3, 'is': 1, 'shining': 2, 'weather': 6, 'sweet': 4, 'and': 0}\n",
      "['and', 'is', 'shining', 'sun', 'sweet', 'the', 'weather']\n",
      "[[0 1 1 1 0 1 0]\n",
      " [0 1 0 0 1 1 1]\n",
      " [1 2 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer()\n",
    "docs = np.array([\n",
    "    'The sun is shining',\n",
    "    'The weather is sweet',\n",
    "    'The sun is shining and weather is sweet'])\n",
    "bag = count.fit_transform(docs)\n",
    "\n",
    "print(count.vocabulary_)\n",
    "print(count.get_feature_names())\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sõnad, mis esinevad peaaegu kõigis dokumentides ei ole tavaliselt   dokumentide eristamise mõttes kuigi informatiivsed. Mõõt idf (*inverse document frequency*)  on logaritm dokumentide arvu $n_d$ suhtest sõna $t$ sisaldavate dokumentide arvu $f_d(t)$ ja on seega seda suurem, mida vähemates dokumentides sõna esineb. Vahel modiitseeritakse seda funktsiooni liites jagajale või jagatavale arvu 1.\n",
    "\n",
    "$$ idf (t) = log \\frac{n_d}{f_d(t)} $$\n",
    "\n",
    "Mõõt **tf-idf** (*term frequency - inverse document frequency*) korrigeerib terminisagedust $tf$ mõõduga $idf$\n",
    "\n",
    "$$ tfidf (t, d) = tf(i, d) \\times idf(t) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.1 , 0.  , 0.41, 0.41, 0.41, 0.  , 0.41])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = bag.toarray()\n",
    "\n",
    "def idf(X):\n",
    "    \"\"\" \n",
    "    Leiame idf  mõõtude vektori kõigile sõnadele kui meie \n",
    "    sagedusmaatriks (bag.toarray()) on X.\n",
    "    \"\"\"\n",
    "    n_d = len(X)\n",
    "    f_d = np.sum(X != 0, axis=0)\n",
    "    #print(f_d)\n",
    "    return np.log(n_d / f_d)\n",
    "\n",
    "idf(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.41, 0.41, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.41, 0.  , 0.41],\n",
       "       [1.1 , 0.  , 0.41, 0.41, 0.41, 0.  , 0.41]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tfidf(X):\n",
    "    \"\"\" \n",
    "    Leiame tfidf  sageduste vektori kõigile sõnadele kui meie \n",
    "    sagedusmaatriks (bag.toarray()) on X.\n",
    "    \"\"\"\n",
    "    return X * idf(X)\n",
    "\n",
    "tfidf(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moodul sklearn pakub selleks klassi [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html). Tulemused erinevad seoses funktsioonide veidi  erinevate definitsioonidega \n",
    "\n",
    "$$ idf (t) = log \\frac{1+n_d}{1+f_d(t)} $$\n",
    "\n",
    "\n",
    "$$ tfidf (t, d) = tf(i, d) \\times (idf(t) + 1) $$\n",
    "\n",
    "ja dokumendi sagedusvektori normaliseerimisega ühikpikkuseks (L2-normaliseerimine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.43, 0.56, 0.56, 0.  , 0.43, 0.  ],\n",
       "       [0.  , 0.43, 0.  , 0.  , 0.56, 0.43, 0.56],\n",
       "       [0.44, 0.53, 0.34, 0.34, 0.34, 0.26, 0.34]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_t = TfidfTransformer()\n",
    "tfidf_t.fit_transform(count.fit_transform(docs)).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='klf'></a>\n",
    "## Logistiline regressioon dokumentide klassifitseerimiseks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Klass [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) lihtsustab tf-idf mõõdu leidmist olles samaväärne  [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) ja [TfidfTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) üksteise järel rakendamisega. Alustuseks leiame hüperparameetrite otsinguga [GrisSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.htm) parimad parameetrid. Kasutame selleks ainult 5000 objekti, muidu oleks see samm väga ajamahukas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:5000, 'review'].values\n",
    "y_train = df.loc[:5000, 'sentiment'].values\n",
    "X_test = df.loc[5000:10000, 'review'].values\n",
    "y_test = df.loc[5000:10000, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(strip_accents=None,\n",
    "                        lowercase=False,\n",
    "                        preprocessor=None)\n",
    "\n",
    "def tokenizer(text):return text.split()\n",
    "def nopunctuation_tokenizer(text):\n",
    "    np_text = text.translate({ord(c):None for c in string.punctuation})\n",
    "    return np_text.split()\n",
    "\n",
    "param_grid = [{'vect__ngram_range': [(1, 1)],\n",
    "               'vect__tokenizer': [tokenizer, nopunctuation_tokenizer],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              {'vect__ngram_range': [(1, 1)],\n",
    "               'vect__tokenizer': [tokenizer, nopunctuation_tokenizer],\n",
    "               'vect__use_idf':[False],\n",
    "               'vect__norm':[None],\n",
    "               'clf__penalty': ['l1', 'l2'],\n",
    "               'clf__C': [1.0, 10.0, 100.0]},\n",
    "              ]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf),\n",
    "                     ('clf', LogisticRegression(random_state=0, solver=\"liblinear\"))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, param_grid,\n",
    "                           scoring='accuracy',\n",
    "                           cv=5,\n",
    "                           verbose=1,\n",
    "                           n_jobs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 120 out of 120 | elapsed:  1.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=False,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_word...\n",
       "                                              <function nopunctuation_tokenizer at 0x00000259DF3229D8>]},\n",
       "                         {'clf__C': [1.0, 10.0, 100.0],\n",
       "                          'clf__penalty': ['l1', 'l2'],\n",
       "                          'vect__ngram_range': [(1, 1)], 'vect__norm': [None],\n",
       "                          'vect__tokenizer': [<function tokenizer at 0x00000259E4522F78>,\n",
       "                                              <function nopunctuation_tokenizer at 0x00000259DF3229D8>],\n",
       "                          'vect__use_idf': [False]}],\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Väga ajamahukas samm\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parimad parameetrid:  {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__tokenizer': <function nopunctuation_tokenizer at 0x00000259DF3229D8>}\n",
      "\n",
      "Täpsus:  0.8638272345530894\n"
     ]
    }
   ],
   "source": [
    "print(\"Parimad parameetrid: \", gs_lr_tfidf.best_params_)\n",
    "print(\"\\nTäpsus: \", gs_lr_tfidf.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seejärel kasutame neid parameetreid ennustava mudeli treenimiseks andmestiku esimese 25 000 objekti peal ja leiame kõige suurema koefitsendiga (kõige kaalukamad) sõnad otsuse tegemisel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=False, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_pattern='...\n",
       "                                 tokenizer=<function nopunctuation_tokenizer at 0x00000259DF3229D8>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=10.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=0,\n",
       "                                    solver='liblinear', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values\n",
    "\n",
    "lr_tfidf.set_params(**gs_lr_tfidf.best_params_)\n",
    "lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teeme 10-kordse ristkontrolli."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV täpsused:  [0.89 0.9  0.89 0.9  0.9  0.9  0.91 0.89 0.89 0.9 ]\n",
      "CV keskmine täpsus: 0.897 +/- 0.005\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(estimator=lr_tfidf,\n",
    "                         X=X_train,\n",
    "                         y=y_train,\n",
    "                         cv=10,\n",
    "                         n_jobs=1)\n",
    "print('CV täpsused: ', scores)\n",
    "print('CV keskmine täpsus: %.3f' % np.mean(scores), \"+/- %.3f\" % np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ja lõpuks kontrollime erinevate meetrikate ja testandmetega."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Täpsus testandmetel: 0.89652\n",
      "F1-skoor testandmetel: 0.896903518909656\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "y_pred = lr_tfidf.predict(X_test)\n",
    "print(\"Täpsus testandmetel:\", accuracy_score(y_test, y_pred))\n",
    "print(\"F1-skoor testandmetel:\", f1_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Koefitsendid: [-3.89e+00 -1.09e-02 -7.82e-04 ... -2.98e-02 -1.30e-01 -1.12e+00]\n"
     ]
    }
   ],
   "source": [
    "coef = lr_tfidf.named_steps['clf'].coef_[0]\n",
    "print(\"Koefitsendid:\", coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sõnastik: 145212\n"
     ]
    }
   ],
   "source": [
    "word_dict = lr_tfidf.named_steps['vect'].vocabulary_\n",
    "print(\"Sõnastik:\", len(word_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Need sõnad mõjutavad arvustuse sentimenti enim (koefitsent, sõna):\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(-18.581550130186805, 'worst'),\n",
       " (-13.953109173385668, 'waste'),\n",
       " (-12.84709346084585, 'awful'),\n",
       " (-12.18989776865045, 'poor'),\n",
       " (-12.071913591071784, 'boring'),\n",
       " (-12.05526823169655, 'bad'),\n",
       " (11.629898736543854, 'great'),\n",
       " (11.217408470171877, 'excellent'),\n",
       " (11.16384230825954, '710'),\n",
       " (-10.625470462105891, 'nothing'),\n",
       " (-10.34525962414036, '410'),\n",
       " (-9.678362088776788, 'worse'),\n",
       " (-9.602211685977508, 'terrible'),\n",
       " (9.383631566601261, 'hilarious'),\n",
       " (-9.142327654057086, 'lame'),\n",
       " (-9.118222935937693, 'horrible'),\n",
       " (8.986600481949363, 'amazing'),\n",
       " (-8.836397005665134, 'poorly'),\n",
       " (-8.718576934806418, 'fails'),\n",
       " (8.631639710122297, 'perfect')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_coef = [(coef[word_dict[w]], w) for w in word_dict]\n",
    "term_coef = sorted(term_coef, key=lambda pair: abs(pair[0]), reverse=True)\n",
    "print(\"Need sõnad mõjutavad arvustuse sentimenti enim (koefitsent, sõna):\\n\")\n",
    "term_coef[:20]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
